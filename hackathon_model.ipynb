{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbtemRu3kDUm"
      },
      "source": [
        "!unzip -uq \"/content/drive/MyDrive/gpu hackathon/test.zip\"\n",
        "!unzip -uq \"/content/drive/MyDrive/gpu hackathon/training.zip\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bx_mryXgbwy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "#from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJY63_Q2n7v_"
      },
      "source": [
        "Image_Width=64\n",
        "Image_Height=64\n",
        "Image_Size=(Image_Width,Image_Height)\n",
        "Image_Channels=3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4CVozBJoDxa",
        "outputId": "ab08e78a-1ef2-489e-9ab5-45623d6075fc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,\\\n",
        "     Dropout,Flatten,Dense,Activation,\\\n",
        "     BatchNormalization\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(Image_Width,Image_Height,Image_Channels)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128,(3,3),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "  optimizer='Adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 62, 62, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 29, 29, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 2,457,026\n",
            "Trainable params: 2,455,554\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD7etBFOoIIH"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "earlystop = EarlyStopping(patience = 10)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',patience = 2,verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
        "callbacks = [earlystop,learning_rate_reduction]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSiBsyyYoNIX",
        "outputId": "40b8b205-de1c-4309-e03f-af576ab55af9"
      },
      "source": [
        "train = ImageDataGenerator(rescale=1/255,validation_split=0.2)\n",
        "test = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_dataset = train.flow_from_directory(\"/content/drive/MyDrive/gpu hackathon/training.zip (Unzipped Files)/training/\",\n",
        "                                          target_size=(64,64),\n",
        "                                          batch_size = 32,\n",
        "                                          subset='training')\n",
        "validation_dataset = train.flow_from_directory(\"/content/drive/MyDrive/gpu hackathon/training.zip (Unzipped Files)/training/\",\n",
        "                                          target_size=(64,64),\n",
        "                                          batch_size = 32,\n",
        "                                          subset='validation')\n",
        "                                         \n",
        "test_dataset = test.flow_from_directory( \"/content/test\",\n",
        "                                          classes=[''],\n",
        "                                          shuffle=False\n",
        "                                          )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4700 images belonging to 2 classes.\n",
            "Found 1175 images belonging to 2 classes.\n",
            "Found 98 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyhFRyv-K0PD",
        "outputId": "5b99e0c0-8873-4b29-a4f5-65b1cf54fa87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkBcwq1yoPbz",
        "outputId": "641ce200-918b-4902-9c7b-8c00b1855562"
      },
      "source": [
        "epochs=100\n",
        "history = model.fit_generator(\n",
        "    train_dataset, \n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=len(validation_dataset),\n",
        "    steps_per_epoch=len(train_dataset),\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "147/147 [==============================] - 1630s 11s/step - loss: 0.5315 - accuracy: 0.7940 - val_loss: 1.6270 - val_accuracy: 0.7574\n",
            "Epoch 2/15\n",
            "147/147 [==============================] - 53s 361ms/step - loss: 0.2526 - accuracy: 0.9111 - val_loss: 0.9689 - val_accuracy: 0.7617\n",
            "Epoch 3/15\n",
            "147/147 [==============================] - 54s 366ms/step - loss: 0.1982 - accuracy: 0.9355 - val_loss: 0.3582 - val_accuracy: 0.8732\n",
            "Epoch 4/15\n",
            "147/147 [==============================] - 54s 365ms/step - loss: 0.1620 - accuracy: 0.9430 - val_loss: 0.2360 - val_accuracy: 0.9055\n",
            "Epoch 5/15\n",
            "147/147 [==============================] - 53s 361ms/step - loss: 0.1274 - accuracy: 0.9572 - val_loss: 0.1844 - val_accuracy: 0.9200\n",
            "Epoch 6/15\n",
            "147/147 [==============================] - 54s 366ms/step - loss: 0.0959 - accuracy: 0.9668 - val_loss: 0.2034 - val_accuracy: 0.9157\n",
            "Epoch 7/15\n",
            "147/147 [==============================] - 53s 358ms/step - loss: 0.0897 - accuracy: 0.9672 - val_loss: 0.2441 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 8/15\n",
            "147/147 [==============================] - 52s 357ms/step - loss: 0.0611 - accuracy: 0.9791 - val_loss: 0.3656 - val_accuracy: 0.8791\n",
            "Epoch 9/15\n",
            "147/147 [==============================] - 52s 357ms/step - loss: 0.0593 - accuracy: 0.9785 - val_loss: 0.3853 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 10/15\n",
            "147/147 [==============================] - 53s 361ms/step - loss: 0.0378 - accuracy: 0.9879 - val_loss: 0.3361 - val_accuracy: 0.8945\n",
            "Epoch 11/15\n",
            "147/147 [==============================] - 52s 357ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.4756 - val_accuracy: 0.8613\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 12/15\n",
            "147/147 [==============================] - 53s 359ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 0.4073 - val_accuracy: 0.8749\n",
            "Epoch 13/15\n",
            "147/147 [==============================] - 53s 359ms/step - loss: 0.0231 - accuracy: 0.9915 - val_loss: 0.5268 - val_accuracy: 0.8519\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 14/15\n",
            "147/147 [==============================] - 53s 358ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.5265 - val_accuracy: 0.8604\n",
            "Epoch 15/15\n",
            "147/147 [==============================] - 53s 359ms/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.5628 - val_accuracy: 0.8562\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIdsuIC-oRz5"
      },
      "source": [
        "model.save(\"97.95_model.h5\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHypiNkvjjtv",
        "outputId": "58869b68-3bc1-42d0-a335-27dacd44c285"
      },
      "source": [
        "import os\n",
        "images = []\n",
        "labels=(np.array(os.listdir('/content/test/'))).T\n",
        "for i in range(len(labels)):\n",
        "  labels[i]=labels[i].split('.')[0]\n",
        "labels=labels.astype(int)\n",
        "\n",
        "for img in os.listdir('/content/test/'):\n",
        "    img = load_img('/content/test/'+img)\n",
        "    img = np.array(img)\n",
        "    img=img/255\n",
        "    img=np.expand_dims(img,axis=1)\n",
        "    images.append(img.reshape(1,64,64,3))\n",
        "images = np.vstack(images)\n",
        "classes = ((model.predict(images).argmax(axis=1))).T\n",
        "\n",
        "final=(np.vstack((labels,classes))).T\n",
        "final = final[final[:, 0].argsort()]\n",
        "print(final)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  1]\n",
            " [ 2  0]\n",
            " [ 3  1]\n",
            " [ 4  0]\n",
            " [ 5  1]\n",
            " [ 6  0]\n",
            " [ 7  1]\n",
            " [ 8  0]\n",
            " [ 9  1]\n",
            " [10  0]\n",
            " [11  1]\n",
            " [12  0]\n",
            " [13  1]\n",
            " [14  0]\n",
            " [15  1]\n",
            " [16  0]\n",
            " [17  1]\n",
            " [18  0]\n",
            " [19  1]\n",
            " [20  1]\n",
            " [21  1]\n",
            " [22  1]\n",
            " [23  1]\n",
            " [24  1]\n",
            " [25  1]\n",
            " [26  1]\n",
            " [27  0]\n",
            " [28  1]\n",
            " [29  1]\n",
            " [30  1]\n",
            " [31  0]\n",
            " [32  0]\n",
            " [33  0]\n",
            " [34  0]\n",
            " [35  0]\n",
            " [36  0]\n",
            " [37  0]\n",
            " [38  0]\n",
            " [39  0]\n",
            " [40  1]\n",
            " [41  0]\n",
            " [42  1]\n",
            " [43  0]\n",
            " [44  1]\n",
            " [45  0]\n",
            " [46  1]\n",
            " [47  0]\n",
            " [48  1]\n",
            " [49  0]\n",
            " [50  1]\n",
            " [51  1]\n",
            " [52  1]\n",
            " [53  1]\n",
            " [54  1]\n",
            " [55  1]\n",
            " [56  0]\n",
            " [57  0]\n",
            " [58  0]\n",
            " [59  0]\n",
            " [60  0]\n",
            " [61  1]\n",
            " [62  1]\n",
            " [63  1]\n",
            " [64  0]\n",
            " [65  0]\n",
            " [66  0]\n",
            " [67  1]\n",
            " [68  1]\n",
            " [69  1]\n",
            " [70  1]\n",
            " [71  1]\n",
            " [72  1]\n",
            " [73  1]\n",
            " [74  0]\n",
            " [75  0]\n",
            " [76  0]\n",
            " [77  0]\n",
            " [78  0]\n",
            " [79  0]\n",
            " [80  0]\n",
            " [81  1]\n",
            " [82  1]\n",
            " [83  1]\n",
            " [84  1]\n",
            " [85  0]\n",
            " [86  0]\n",
            " [87  0]\n",
            " [88  0]\n",
            " [89  0]\n",
            " [90  1]\n",
            " [91  1]\n",
            " [92  1]\n",
            " [93  1]\n",
            " [94  1]\n",
            " [95  0]\n",
            " [96  0]\n",
            " [97  0]\n",
            " [98  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLdl5SDASjX"
      },
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "\n",
        "def write_json(filename, result):\n",
        "    with open(filename, 'w') as outfile:\n",
        "        json.dump(result, outfile)\n",
        "\n",
        "def read_json(filename):\n",
        "    with open(filename, 'r') as outfile:\n",
        "        data =  json.load(outfile)\n",
        "    return data\n",
        "\n",
        "def generate_sample_file(filename):\n",
        "    res = {}\n",
        "    for i in final:\n",
        "        test_set = str(i[0]) + '.jpg'\n",
        "        res[test_set] = int(i[1])\n",
        "\n",
        "    write_json(filename, res)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    generate_sample_file('./sample_result1.json')\n"
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}